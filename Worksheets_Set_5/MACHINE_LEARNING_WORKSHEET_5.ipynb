{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING WORKSHEET - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 to Q15 are subjective answer type questions, Answer them briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. R-squared or Residual Sum of Squares (RSS) which one of these two is a better measure of goodness of fit model in regression and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     R-squared is a better measure of goodness of fit model in regresseion as it provides a useful measure of how well a \n",
    "     model fits, in terms of (squared) distance from points to the best fitting line. Thus, R-squared acts as an \n",
    "     evaluation metric to evaluate the scatter of the data points around the fitted regression line. It recognizes \n",
    "     the percentage of variation of the dependent variable.  However, as one adds more regression coefficients, R2 never \n",
    "     goes down, even if the additional X variable is not useful.\n",
    "     \n",
    "     The value of R-squared stays between 0 and 100%: \n",
    "        - 0% corresponds to a model that does not explain the variability of the response data around its mean. The mean of\n",
    "          the dependent variable helps to predict the dependent variable and also the regression model. \n",
    "        - On the other hand, 100% corresponds to a model that explains the variability of the response variable around its \n",
    "          mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. What are TSS (Total Sum of Squares), ESS (Explained Sum of Squares) and RSS (Residual Sum of Squares) in regression. Also mention the equation relating these three metrics with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    TSS (Total Sum of Squares) - The total sum of squares is a variation of the values of a dependent variable from the \n",
    "                                 sample mean of the dependent variable. Essentially, the total sum of squares quantifies \n",
    "                                 the total variation in a sample.\n",
    "                                 It can be determined using the following formula:\n",
    "                                                              TSS = Σ(yi – mean of ȳ)^2\n",
    "                                 where,\n",
    "                                 yi – the value in a sample\n",
    "                                 ȳ – the mean value of a sample\n",
    "                                 \n",
    "                                 \n",
    "     ESS (Explained Sum of Squares) - The ESS is the portion of total variation that measures how well the regression \n",
    "                                      equation explains the relationship between X and Y. Thus, it tells about how much of \n",
    "                                      the variation in the dependent variable your model explained.\n",
    "                                      You compute the ESS with the formula :\n",
    "                                                  ESS = Σ(yi – ŷi)^2.\n",
    "                                      Where:\n",
    "                                      yi – the observed value\n",
    "                                      ŷi – the value estimated by the regression line                                 \n",
    "                                 \n",
    "     RSS (Residual Sum of Squares) - A residual sum of squares (RSS) measures the level of variance in the error term, \n",
    "                                     or residuals, of a regression model.Generally, a lower residual sum of squares \n",
    "                                     indicates that the regression model can better explain the data while a higher residual \n",
    "                                     sum of squares indicates that the model poorly explains the data. \n",
    "                                     The residual sum of squares can be found using the formula below:\n",
    "                                                       RSS = Σ(yi - f(xi))^2\n",
    "                                     where,\n",
    "                                     yi = the ith value of the variable to be predicted\n",
    "                                     f(xi) = predicted value of yi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. What is the need of regularization in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Regularization is important in machine learning as it is a form of regression, that constrains/ regularizes or shrinks \n",
    "    the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or\n",
    "    flexible model, so as to avoid the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. What is Gini–impurity index? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, \n",
    "    if that new instance were randomly classified according to the distribution of class labels from the data set. Gini \n",
    "    impurity is lower bounded by 0, with 0 occurring if the data set contains only one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Are unregularized decision-trees prone to overfitting? If yes, why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Yes, Decision trees are prone to overfitting, especially when a tree is particularly deep. This is due to the amount \n",
    "    of specificity we look at leading to smaller sample of events that meet the previous assumptions. This small sample\n",
    "    could lead to unsound conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6. What is an ensemble technique in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ensemble methods is a machine learning technique that combines several base models in order to produce one \n",
    "    optimal predictive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. What is the difference between Bagging and Boosting techniques? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging: \n",
    "\n",
    "    Bagging is also known as bootstrap aggregating sits on top of the majority voting principle. The samples are \n",
    "    bootstrapped each time when the model is trained. When the samples are chosen, they are used to train and validate \n",
    "    the predictions. The samples are then replaced back into the training set. The samples are selected at random. \n",
    "    This technique is known as bagging. To sum up, base classifiers such as decision trees are fitted on random subsets of \n",
    "    the original training set. Subsequently, the individual predictions are aggregated (voting or averaging etc.). The \n",
    "    final results are then used as predictions. It reduces the variance of a black box estimator. Due to this the chances \n",
    "    of overfitting is ruled out.\n",
    "    \n",
    "    \n",
    "Boosting:\n",
    "\n",
    "    The concept of Adaptive Boost revolves around correcting previous classifier mistakes. Each classifier gets trained on \n",
    "    the sample set and learns to predict. The misclassification errors are then fed into the next classifier in the chain \n",
    "    and are used to correct the mistakes until the final model predicts accurate results. When a weak-classifier \n",
    "    misclassifies a training sample, the algorithm then uses these very samples to improve the performance of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. What is out-of-bag error in random forests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The out-of-bag (OOB) error, also called out-of-bag estimate, is the average error for each calculated using predictions \n",
    "    from the trees that do not contain in their respective bootstrap sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9. What is K-fold cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Cross validation (CV) is one of the technique used to test the effectiveness of a machine learning models, it is also a \n",
    "     re-sampling procedure used to evaluate a model if we have a limited data.\n",
    "     The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be \n",
    "     split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it \n",
    "     may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. What is hyper parameter tuning in machine learning and why it is done? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for \n",
    "    the ideal model architecture is referred to as hyperparameter tuning.\n",
    "    Hyperparameters are crucial as they control the overall behaviour of a machine learning model. Every machine learning \n",
    "    models will have different hyperparameters that can be set. A hyperparameter is a parameter whose value is set before \n",
    "    the learning process begins.The ultimate goal is to find an optimal combination of hyperparameters that minimizes \n",
    "    a predefined loss function to give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. What issues can occur if we have a large learning rate in Gradient Descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution. Thus, When \n",
    "    the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 12. Can we use Logistic Regression for classification of Non-Linear Data? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space \n",
    "    to separate observations that belong to a class from all the other observations that do not belong to that class. \n",
    "    The decision boundary is thus linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 13. Differentiate between Adaboost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The main differences, therefore, are that Gradient Boosting is a generic algorithm to find approximate solutions to\n",
    "    the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function.\n",
    "    \n",
    "Gradient Boosting - \n",
    "\n",
    "    It calculates the gradient (derivative) of the Loss Function with respect to the prediction (instead of the \n",
    "    features). Gradient boosting increases the accuracy by minimizing the Loss Function (error which is difference of actual \n",
    "    and predicted value) and having this loss as target for the next iteration. Gradient boosting algorithm builds first \n",
    "    weak learner and calculates the Loss Function. It then builds a second learner to predict the loss after the first step. \n",
    "    The step continues for third learner and then for fourth learner and so on until a certain threshold is reached.\n",
    "    \n",
    "    \n",
    "Adaboost -\n",
    "\n",
    "    Adaboost is more about ‘voting weights’ and Gradient boosting is more about ‘adding gradient optimization’. \n",
    "    Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At \n",
    "    each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of \n",
    "    the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly \n",
    "    predicted instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 14. What is bias-variance trade off in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The bias is known as the difference between the prediction of the values by the ML model and the correct value. It is \n",
    "    the simplifying assumptions made by the model to make the target function easier to approximate.\n",
    "    \n",
    "    The variability of model prediction for a given data point which tells us spread of our data is called the variance of \n",
    "    the model. Variance is the amount that the estimate of the target function will change given different training data.\n",
    "    \n",
    "    Thus, Trade-off is tension between the error introduced by the bias and the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 15. Give short description each of Linear, RBF, Polynomial kernels used in SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the\n",
    "    required form.\n",
    "    \n",
    "    \n",
    "    Linear Kernel - A linear kernel can be used as a normal dot product any two given observations The product between \n",
    "                    two vectors is the sum of the multiplication of each pair of input values.\n",
    "                                                   K(x, xi) = sum(x*xi)\n",
    "                                                \n",
    "                                                \n",
    "    RBF Kernel - Radial Basis Function (RBF) Kernel is a popular kernel function commonly used in SVM classification. RBF \n",
    "                 can map an input space into infinite dimensional space.\n",
    "                                                  K(x, xi) = exp(-gamma* sum(x - xi^2))\n",
    "                 Here, gamma is a parameter which ranges from 0 to 1.\n",
    "    \n",
    "    \n",
    "    Polynomial Kernel - A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel \n",
    "                        can distinguish curved or nonlinear input space.\n",
    "                                                   K(x, xi) = 1 + sum(x*xi)^d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
